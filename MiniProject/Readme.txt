ğŸ“¦ Configurable Data Ingestion & Processing Pipeline
ğŸ“Œ Overview

This project is a real-world, end-to-end backend data pipeline that ingests data from multiple sources, processes it through a configurable pipeline, and produces metrics.

It demonstrates advanced Python fundamentals commonly used in:

Data Engineering

ML Feature Pipelines

Backend & Microservices

Async Systems

ğŸ¯ Key Objectives

Fetch data concurrently from multiple sources using async/await

Process data using a composition-based pipeline

Apply logging and timing decorators without modifying business logic

Avoid mutable shared state bugs

Use Pythonic comprehensions for transformations

Build a system that is easy to extend and interview-ready

ğŸ§  Concepts Demonstrated
Topic	How itâ€™s Used
Python Internals & Mutability	Demonstrates mutation bugs and safe copy patterns
List & Dict Comprehensions	Data filtering, feature engineering, metrics
OOP Fundamentals	Clean classes, __init__, single responsibility
Composition over Inheritance	Pipeline accepts pluggable processing steps
Decorators	Logging and timing as cross-cutting concerns
Async Programming	Concurrent ingestion using asyncio.gather
ğŸ—ï¸ Project Structure
data_pipeline_project/
â”‚
â”œâ”€â”€ decorators.py      # Logging & timing decorators
â”œâ”€â”€ ingestion.py       # Async data ingestion layer
â”œâ”€â”€ processors.py      # Data cleaning, transformation & features
â”œâ”€â”€ pipeline.py        # Composition-based pipeline orchestration
â”œâ”€â”€ main.py            # Entry point (end-to-end execution)
â””â”€â”€ README.md          # Project documentation

ğŸ§© Architecture Diagram
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ External Data APIs â”‚
â”‚ (Source A, B, ...) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚   async / await
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ingestion Layer   â”‚
â”‚  (asyncio.gather)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚   raw data (list of dicts)
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Pipeline      â”‚
â”‚ (Composition-based)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚   Processing Steps (Order)   â”‚
 â”‚                              â”‚
 â”‚  1. Cleaner                  â”‚
 â”‚  2. Transformer              â”‚
 â”‚  3. FeatureEngineer          â”‚
 â”‚  4. MetricsCalculator        â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Final Output      â”‚
â”‚  (Metrics / Data)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ”„ Execution Flow

Async ingestion

Multiple data sources are fetched concurrently

Reduces total latency (I/O-bound optimization)

Pipeline execution

Data flows through each processor step

Each step:

Receives data

Returns new data

Does not mutate shared state

Decorators

Logging and timing applied transparently

No clutter inside business logic

ğŸ§ª Example Data Flow
Raw Data (Ingested)
[
  {"id": 1, "value": 10},
  {"id": 2, "value": None},
  {"id": 3, "value": 30}
]

After Cleaning
[
  {"id": 1, "value": 10},
  {"id": 3, "value": 30}
]

After Feature Engineering
[
  {"id": 1, "value": 20, "normalized_value": 0.33},
  {"id": 3, "value": 60, "normalized_value": 1.0}
]

Final Metrics
{
  "count": 2,
  "min": 20,
  "max": 60,
  "avg": 40.0
}

ğŸ§  Design Decisions (Important)
Why Composition?

Steps can be added, removed, or reordered

No fragile inheritance hierarchies

Pipeline logic never changes

Inheritance models identity; composition models behavior

Why Decorators?

Logging and timing are cross-cutting concerns

Avoids duplicated code

Keeps business logic clean

Why Async?

External APIs and databases are slow

Async allows concurrent waiting

Improves throughput without threads

â–¶ï¸ How to Run
python main.py


Requirements:

Python 3.9+

No external dependencies

ğŸ¯ Interview-Ready Explanation (Use This)

I built a configurable async data ingestion and processing pipeline using composition over inheritance. Data is fetched concurrently using async/await, processed through independent pipeline steps, and instrumented with decorators for logging and timing. The design avoids shared mutable state, uses Pythonic comprehensions, and is easily extensible.